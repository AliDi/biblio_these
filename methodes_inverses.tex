\section{Les méthodes inverses}
En formation de voies, chaque source est considérée indépendemment des autres. La surface contenant les sources potentielles est scannées point par point et l'éventuelle cohérence des sources n'est pas prise en compte.\\

L'approche des méthodes inverses est de traiter le problème dans son ensemble, en recherchant toutes les sources simultanément, prenant ainsi en compte les effets d'interférence entre les sources. \todo[inline]{Quid des interactions non-linéaires entre sources ?} La résolution du problème inverse ne peut généralement pas reposer sur une inversion de la matrice de transfert, car le problème inverse est souvent sous-déterminé. De plus, la relation entre sources et mesures n'est pas toujours bijective.

Les méthodes varient selon le modèle de source choisi : \\
-ondes planes propagatives et évanescentes : NAH, SONAH\\
-radiation BEM\\
-distribution de monopoles\\
-harmoniques sphériques.
 

itératif ou non ?

Holographie en champ proche : quand il est possible d'effectuer des mesures en champ proche, les signaux de mesures sont porteurs de plus d'information, notamment les ondes évanescentes.


%Reconstruction pixellisée des paramètres ou déformation de contour.

\subsection{Holographie en champ proche}
%hypothèse : on connait le plan d'émission des sources + les fonctions de transferts
L'holographie en champ proche propose d'exploiter des mesures réalisées à proximité des sources pour en reconstruire une image. Cette méthode tire profit de la mesure des ondes évanescentes, exponentiellement décroissantes avec la distance, qui viennent s'ajouter aux ondes propagatives \cite{Maynard1985}.\\
Le champ de pression mesuré est d'abord décomposé dans le domaine des nombres d'ondes par une transformée de Fourier spatiale. A chaque onde est associé un propagateur (i.e. une fonction de transfert supposée connue) qui, inversé, permet de rétropropager le champ mesuré et ainsi reconstruire le champ source.  Avant rétropropagation, un filtre sur les nombres d'ondes est appliqué de manière à sélectionner les nombres d'ondes d'intérêt : il est nécessaire de trouver un compromis permettant de conserver suffisamment d'ondes évanescentes (porteuses d'informations) tout en limitant l'amplification du bruit. La mesure à proximité permet ainsi d'obtenir une résolution supérieure à la demi-longueur d'onde.\\

Amélioration SONAH pour éviter de faire la transformée de Fourier spatiale %J. Hald. Time domain acoustical holography and its applications. Sound and Vibration,
 + M-SONAH dans le cas où on ne connaît pas analytiquement toutes les ondes pour décrire le champ d'onde complet : elles sont alors alors exprimer comme étant une combinaison des différentes ondes.\\
 
+ amélioration sur des géométrie quelconques par l'utilisation d'éléments de frontières (iBem : méthode des éléments de frontières inverse). La transformée de Fourier spatiale est remplacée par une SVD de la matrice de transfert (ref 29 thèse de T. Lemagueresse). L'équation liant la pression pariétale à  la pression mesurée est alors résolue par la méthode des éléments de frontière. \\
 
+ en espace clos\\

+ antenne double couche

%voir http://www.conforg.fr/cfa2006/cdrom/data/articles/000175.pdf

\todo[inline]{
%\subsection{Méthodes par mise à jour successive d'un modèle}
L'idée est de minimiser l'écart entre les signaux mesurés et ceux calculés à partir d'un modèle.

The polar correlation technique : JET ENGINE NOISE SOURCE LOCATION: 
THE 
POLAR 
CORRELATION 
TECHNIQUE 
M. 
J. 
FISHER

%très proche : P. J. T. FILLIPI, D. HABAULT and J. PIRAUX 1988 Journal of Sound and <ibration 124, 285}296.
Noise source modelling and identi"cation procedure

}

\subsection{Helmoltz equation least squares}


\subsubsection{Choix de la fonction coût}

Il existe trois principales approches pour poser le problème d'optimisation et en définir une fonction objectif \footnote{détails des calculs dans le cours de E. Thiébaut et C. Pichon : \url{https://cral.univ-lyon1.fr/labo/perso/eric.thiebaut/downloads/documents/cargese-2006-thiebaut.pdf}} : 

\paragraph{\tbullet le maximum de vraisemblance}, pour lequel on cherche la solution 
	\begin{equation}
		\bm{q}_{ML} = \arg\max_{\bm{q}} [ \bm{p}|\bm{q} ]=\arg\min_q f_{data},
	\end{equation}	
	 avec 
	\begin{eqnarray}
		f_{data} &=&-\log[\bm{p}|\bm{q}] + \text{cst}\\
				 &=& [\bm{p}-\bm{Gq}]'  \bm{C}_{\bm{p}|\bm{q}}^{-1}  [\bm{p}-\bm{Gq}]
	\end{eqnarray}
	en considérant que $[ \bm{p}|\bm{q} ]$ a une répartition gaussienne, de covariance $\bm{C}_{\bm{p}|\bm{q}}$ . L'estimateur obtenu est le même que celui donné par la méthode des moindres carrés.\\
	 	
\paragraph{\tbullet le maximum a posteriori} (MAP), qui cherche à maximiser la densité des sources a posteriori : 
	\begin{eqnarray}
		\bm{q}_{MAP} &=& \arg\max_{\bm{q}}[ \bm{q} |  \bm{p}  ]\\
					& =& \arg\max_{\bm{q}}([\bm{p} | \bm{q}][\bm{q}])\\
					&=& \arg\min_{\bm{q}} ( \underbrace{-\log[\bm{p}|\bm{q}]}_{f_{data}} \underbrace{- \log[\bm{q}]}_{f_{prior}})
	\end{eqnarray}
	La méthode MAP propose donc d'ajouter à au maximum de vraisemblance un terme de régularisation donné par $f_{prior}$. Considérant que les sources ont une densité de probabilité gaussienne, le terme de régularisation s'écrit : 
	\begin{equation}
		f_{prior}= (\bm{q}- \bm{\bar{q}})' \bm{C_q}^{-1}(\bm{q}- \bm{\bar{q}})
	\end{equation}\\	
	
\paragraph{\tbullet le minimum de variance} \footnote{\textit{minimum mean square error}}, dont le critère est de minimiser l'espérance de l'erreur quadratique : 
	\begin{equation}
		\bm{q}_{MV} = \arg\min_{\tilde{q}} \mathbb{E}\{||\bm{q}-\bm{\tilde{q}}||^2\}
	\end{equation}	
	Cette solution généralise le filtre de Wiener et est la même que la solution MAP. \\


En faisant un analyse de la propagation des erreurs pour chaque méthode, on peut montrer que le maximum de vraisemblance en propage davantage.\\

En résumé, les méthodes inverses utilisées principalement sont de 2 sortes : 1) les méthodes basées sur la transformée de Fourier (holographie,...) ; 2) Les méthodes ``model based''. En pratiques, ces méthodes sont très proches (ce que montrent %W. A. VERONESI and J. D. MAYNARD 1989 Journal of the Acoustical Society of America 85,588}598. Digital holographic reconstruction of sources with arbitrarily shaped surfaces)





//sources équivalentes (ESM)

\subsection{Les méthodes de régularisation}

liste des méthodes de régularisation : \url{http://www.imm.dtu.dk/~pcha/Regutools/RTv4manual.pdf}


nelson part2 compare deux méthodes de régularisation : 1) il explique comment choisir un paramètre de Tikhonov ; 2) il explique comment choisir les valeurs de la SVD à supprimer.\\ Beaucoup se sont penchés sur le problème du choix de $\eta$.nelson part2 compare les différentes facçon de déterminer $\eta$ en comparant l'erreur entre le champ source désiré et celui reconstruit, ainsi que l'erreur entre l'interspectre reconstruit et le vrai interspectre (les $S_{qq}$). Les méthodes comparées sont : \\
-cross-validation technique : %M. ALLEN 1974 ¹echnometrics 16, 125}127. The relationship between variable selection and data
augmentation and a method for prediction\\
-generalize cross-validation technique : %G. H. GOLUB, M. HEATH and G. WAHBA 1979 ¹echnometrics 21, 215}223. Generalized crossvalidationas a method for choosing a good ridge parameter






Les problèmes inverses de localisation de sources acoustiques sont souvent mal posés car le nombre de sources est supérieur au nombre de capteur (la solution n'est pas unique) et la solution dépend des données d'entrée. Il est alors nécessaire de mettre en place des stratégies qui améliorent le conditionnement du problème, notamment en réduisant la sensibilité de la solution aux données d'entrée.\\


\subsubsection{Décomposition en valeurs singulières}


Le conditionnement du problème peut être quantifié par le rapport entre la plus grande et la plus petite valeur singulière de la matrice de transfert. Plus ce rapport est faible, mieux le problème est conditionné. Le conditionnement du problème peut donc être amélioré en supprimant les petites valeurs singulières de la matrice de transfert. La question du nombre de valeurs singulières à conserver se pose alors. 



\subsubsection{Régularisation de Tikhonov}

La stratégie la plus souvent adoptée est la régularisation de Thikonov \citep{Tikhonov1963} qui consiste à rajouter un terme de contrôle de l'énergie de la solution dans la fonction coût. Cette dernière prend alors la forme suivante : 
\begin{equation}
	||\bm{p}-\bm{G}\bm{q}||^2 + \eta^2||\bm{q}||^2
\end{equation}
où $||\bullet||$ est la norme euclidienne et $\eta^2$ est le paramètre de régularisation, choisi judicieusement de façon à favoriser les solutions de petite norme.

La difficulté de cette régularisation réside dans ne choix de $\eta$. Ce paramètre peut être déterminé par des procédures ad-hoc qui telle que : \\
-discrepancy principle\\
-general cross-validation (méthode de la validation croisée généralisée)\\
-L-curve method : %P. C. HANSEN and D. P. O'LEARY 1993 SIAM Journal on Scienti,c Computing 14, 1487}1503. The use of the L-curve in the regularisation of discrete ill-posed problems\\
-(restricted) maximum likehood : \\%B. ANDERSSEN and P. BLOOMFIELD 1974 Numerische Mathematik 22, 157}182. Numerical
differentiation procedures for non-exact data\\
-unbiased predictive risk estimator\\
-interprétation bayesienne \citep{Pereira2015}\\
- méthode utilisant le principe d’anomalie de Morozov\\
-normalized cumulative periodogramm\\
-...\\

\subsubsection{Optimisation parcimonieuse }
L'objectif d'une approche parcimonieuse est d'obtenir une solution approchée du problème avec le moins de composantes non nulles possible. On minimise alors à la fois l'écart entre les données mesurées et simulées, ainsi que la "norme" $L_0$ qui donne la parcimonie d'un vecteur $\bm{x}$ telle que : $||\bm{x}||_0 := \#\{i|x_i\neq0\}$. Le problème d'optimisation devient alors bi-objectif : 
\begin{equation}
	\min_{\bm{q}}(||\bm{q}||_0 , ||\bm{p}-\bm{G}\bm{q}||^2) .
	\label{bi-objectif}
\end{equation}
Prendre en compte une distribution parcimonieuse des sources dans l'espace, par exemple, permet de réduire le caractère sous-déterminé du problème en exploitant les connaissances a priori sur les sources. Cette propriété de parcimonie sert notamment à compenser le rayonnement omnidirectionnel des sources qui n'est pas mesuré et qui engendre une sous-estimation du niveau des sources. \\
La parcimonie est donnée par la norme $L_0$ du champ source (qui donne alors le nombre de valeurs non-nulles de $\bm{G_{qq}}$. Un formalisme bayesien permet de prendre en compte cette parcimonie en définissant une densité de probabilité des sources $[\bm{p}]$. Une loi gaussienne peut par exemple être choisie telle que : 
\begin{equation}
 [\bm{p}] \propto \exp\left(\frac{\sum_i |q_i|^p}{2\gamma^2}\right)
\end{equation}
avec $i$ le i\textsuperscript{ème} élément de $\bm{q}$. Dans cette formulation, la norme $L_0$ peut être relaxée par une norme $L_p$ permettant de rendre l'objectif convexe, avec $p$ un paramètre prenant une valeur entre $0$ et $2$. $p=0$ correspond à une distribution parcimonieuse, tandis que plus $p$ tend vers $2$, plus la distribution spatiale des source est étendue.

\cite{Tropp2010} passent en revue les principales façons de poser et de résoudre ce problème d'optimisation.\\
% p=0
%------------------------------
Lorsque le paramètre $p$ est proche de 0, le critère n'est pas convexe et le problème doit être résolu à l'aide d'algorithmes gloutons, dont les plus répandus sont décrits ci-dessous : 
\paragraph{\tbullet Matching pursuit (MP)} Minimiser une fonction coût de la forme $||\bm{p}-\bm{Gq}||_2$ avec une contrainte de parcimonie $||\bm{q}||_0 \leq \epsilon$ peut être vu comme une sorte d'analyse en composante principale de $\bm{p}$, par une projection sur un ensemble d'atome (pas forcément orthogonaux) trié dans $\bm{G}$, où $\bm{q}$ donne l'amplitude pour chaque atome. \cite{Mallat1993} propose un algorithme qui calcule successivement  à partir d'un dictionnaire d'atomes normalisés les poids associés aux atomes pour lesquels le produit scalaire avec le signal est maximal. L'opération est répétée sur les résidus jusqu'à ce que le signal soit suffisamment décomposé, i.e. qu'un critère sur les résidus soit atteint.

\paragraph{\tbullet Orthogonal matching pursuit (OMP)} Une extension de l'algorithme MP propose également d'extraire un à un les atomes et leur coefficient, mais à chaque sélection d'atome, la projection du signal dans le nouvel espace vectoriel généré est recalculée, ce qui permet une convergence plus rapide, moyennant une étape d’orthogonalisation supplémentaire \citep{Pati1993}. Chaque atome n'est sélectionné qu'une fois, contrairement à l'algorithme MP. Cette minimisation des redondances également de réduire l'erreur commise. \\

%p=1
%--------------------------------

Il est possible de s'affranchir de la norme $L_0$ en relaxant le paramètre $p$, et en prenant par exemple $p=1$\footnote{$||a||_1=\sum_i |a_i|$} (critère non dérivable). Le problème d'optimisation contenant une contrainte en norme $L_1$ peut s'exprimer de différence manière :  
%Candès montre que la norme L0 peut être équivalente à la norme L1 dans certain cas. 
\paragraph{\tbullet  Poursuite de base (Basis pursuit, BP)} Ce principe d'optimisation s'écrit sous la forme :
\begin{equation}
\min_{\bm{q}} ||\bm{q}||_1 ~~~~\text{sous la contrainte}~~~~ \bm{Gq}=\bm{p}
\end{equation}
Ce problème peut être linéarisé puis résolu par des algorithmes comme ceux du simplexe  ou de points intérieurs \citep{Chen2001}.

\paragraph{\tbullet  Least absolute shrinkage and selection operator (LASSO)} \cite{Tibshirani1996} propose de résoudre :
\begin{equation}
\min(||\bm{p}-\bm{Gq}||^2) ~~~~\text{sous la contrainte}~~~~||\bm{q}||_1\leq t),
\end{equation}
ce qui revient à estimer $\tilde{q}$ tel que : 
\begin{equation}
\bm{\tilde{q}} = \arg\min_{\bm{q}} \left( ||\bm{p}- \bm{Gq}||^2 + \beta ||\bm{q}||_1 \right)
\end{equation}
Quand $\beta=0$, le problème LASSO est analogue aux moindres carrés ordinaires. Si $\beta$ est très grand, $\bm{\tilde{q}}$ tend vers 0. Ce paramètre permet donc de fixer certain coefficients de la régression à 0 ou, avec une approche bayésienne, on peut leur associer une incertitude.

\paragraph{\tbullet  Basis pursuit denoising (BPDN)} Le principe de BPDN mène au même problème que celui formulé par LASSO. On cherche à résoudre :
\begin{equation}
\min_{\bm{q}} ||\bm{q}||_1 ~~~~\text{sous la contrainte}~~~~ ||\bm{Gq}-\bm{p}||^2 \leq \tau,
\end{equation}
ce qui équivaut, comme LASSO à trouver un compromis entre réduire les résidus et trouver la solution la plus parcimonieuse possible.

%0<p<1
%-----------------------------------

~\\Parmi les algorithmes de résolution des problèmes pour $0<p<2$, on trouve : 
\begin{itemize}
	\item[-] les algorithmes de relaxation (ex : RELAX, Li \& Stoica, 1996),
	\item[-] les algorithmes de type "seuillage itératif" (type FISTA, Expectation-Maximisation,...),
	\item[-] IRLS (iterative reweighted least squares) : cette méthode propose de représenter une norme $L_p$ ($0<p\leq1$) par une norme $L_2$ pondérée.  %iterative thresholding algorithm for linear inverse pb with sparsity, daubechies 2004
	 Elle ne garantie pas la convergence vers un minimum global. Elle s'utilise donc plutôt en optimisation locale. Voir par exemple l'algorithme FOCUSS (FOcal Underdetermined System Solver).
	\item[-] least-Angle regression stagewise (LARS) : méthode par homotopie (Osbourne, 2000),
	\item[-] shooting algorithm (Fu, 1998),
	\item[-] gradient conjugué et ses dérivés,
	\item[-] et tous les autres algorithmes d'optimisation convexe quadratique.
\end{itemize}

Si $p>1$, le critère est strictement convexe (et ne présente donc qu'un minimum global). Dans le cas où $p=2$, le problème n'est pas soumis à une contrainte de parcimonie et correspond à la régularisation de Tikhonov ou régression d'arête (\textit{ridge regression}).


\paragraph{Cours et algorithmes liés à l'optimisation parcimonieuse en ligne :}~\\
Une liste de solvers selon la catégorie du problème se trouve à l'adresse : \url{https://web.archive.org/web/20150502191143/http://www.ugcs.caltech.edu/~srbecker/wiki/Category:Solvers}.\\
Cours d'H. Carfantan sur l'optimisation parcimonieuse : \url{http://www.ast.obs-mip.fr/users/carfan/PPF-PSI/CarfantanSparse.pdf}
10 cours  "Sparse Representations and Signal Recovery (Purdue University)", StudentLecture : url{https://engineering.purdue.edu/ChanGroup/ECE695Notes/}



De manière générale, les méthodes de régularisation se confronte aux problématiques suivantes : 
\begin{itemize}
	\item[-] Comment choisir la base de décomposition optimale ?
	\item[-] Comment régler le paramètre $\eta$
	\item[-] Quelle formulation et quel algorithme de résolution choisir ?
	\item[-] Comment évaluer la fiabilité de la solution ?
\end{itemize}



\todo[inline]{Faire les parallèles : \\
CLEAN : technique de déconvolution itérative, heuristique de type "matching pursuit"\\
 SC-DAMAS : de type "basis pursuit"\\
}




